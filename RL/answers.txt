==============TRY 1=======================================================================================================================

reward = 1 - (((self.target_pos - self.sim.pose[:3])).sum()/(self.target_pos.sum()))

self.buffer_size = 100000
self.batch_size = 32

self.gamma = 0.99  # discount factor
self.tau = 0.001  # for soft update of target parameters

lr=0.01

reward = 1 -((abs(self.sim.pose[:3] - self.target_pos)).sum()/self.target_pos.sum())

#Actor

        #Hidden Layers
        net = layers.Dense(units=128,kernel_regularizer=layers.regularizers.l2(1e-6))(states)
        net = layers.BatchNormalization()(net)
        net = layers.Activation("relu")(net)
        
        net = layers.Dense(units=64,kernel_regularizer=layers.regularizers.l2(1e-6))(net)
        net = layers.BatchNormalization()(net)
        net = layers.Activation("relu")(net)
        
        net = layers.Dense(units=32,kernel_regularizer=layers.regularizers.l2(1e-6))(net)
        net = layers.BatchNormalization()(net)
        net = layers.Activation("relu")(net)

        #Raw Values for Actions
        action_vals = layers.Dense(units=self.action_size, activation='sigmoid',
            name='action_values',kernel_initializer=layers.initializers.RandomUniform(minval=-0.003, maxval=0.003))(net)
		
#Critic

        Snet = layers.Dense(units=512,kernel_regularizer=layers.regularizers.l2(1e-6))(states)
        Snet = layers.BatchNormalization()(Snet)
        Snet = layers.Activation("relu")(Snet)

        Snet = layers.Dense(units=256, kernel_regularizer=layers.regularizers.l2(1e-6))(Snet)

        # Add hidden layer(s) for action pathway
        Anet = layers.Dense(units=256,kernel_regularizer=layers.regularizers.l2(1e-6))(actions)

		

Episode =  500, score =   8.937 (best =  75.957 , worst = -102.040))

		
==============TRY 1=======================================================================================================================

==============TRY 2=======================================================================================================================
reward = 1.0 -0.01*((abs(self.sim.pose[:3] - self.target_pos)).sum())

Episode =  500, score = 136.808 (best = 143.800 , worst =   1.800)

==============TRY 2=======================================================================================================================


==============TRY 3=======================================================================================================================

reward = np.tanh((abs(self.sim.pose[:3] - self.target_pos)).sum())

Episode =  500, score =  69.000 (best = 252.000 , worst =   3.000)

==============TRY 3=======================================================================================================================


==============TRY 4=======================================================================================================================


==============TRY 4=======================================================================================================================