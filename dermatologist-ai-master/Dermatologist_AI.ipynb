{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\py36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Dense,GlobalAveragePooling2D\n",
    "from keras.applications import MobileNet\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.mobilenet import preprocess_input\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\py36\\lib\\site-packages\\keras_applications\\mobilenet.py:207: UserWarning: MobileNet shape is undefined. Weights for input shape (224, 224) will be loaded.\n",
      "  warnings.warn('MobileNet shape is undefined.'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.6/mobilenet_1_0_224_tf_no_top.h5\n",
      "17227776/17225924 [==============================] - 11s 1us/step\n"
     ]
    }
   ],
   "source": [
    "#Discards the last layer while importing MobileNet\n",
    "\n",
    "base_model=MobileNet(weights='imagenet',include_top=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding my own fully connected layers\n",
    "\n",
    "x=base_model.output\n",
    "x=GlobalAveragePooling2D()(x)\n",
    "x=Dense(512,activation='relu')(x) #dense layer 1\n",
    "x=Dense(512,activation='relu')(x) #dense layer 2\n",
    "x=Dense(128,activation='relu')(x) #dense layer 3\n",
    "pred=Dense(3,activation='softmax')(x) #final layer with softmax activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create new model based on modifications\n",
    "\n",
    "my_model=Model(inputs=base_model.input,outputs=pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make all layers except FC-layers non-trainable\n",
    "\n",
    "for layer in my_model.layers[:-3]:\n",
    "    layer.trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen=ImageDataGenerator(preprocessing_function=preprocess_input) \n",
    "\n",
    "train_generator=train_datagen.flow_from_directory('data/train/',\n",
    "                                                 target_size=(224,224),\n",
    "                                                 color_mode='rgb',\n",
    "                                                 batch_size=32,\n",
    "                                                 class_mode='categorical',\n",
    "                                                 shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 600 images belonging to 3 classes.\n",
      "Found 150 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "#Pre-processing testing and validation data\n",
    "\n",
    "test_datagen=ImageDataGenerator(preprocessing_function=preprocess_input) \n",
    "\n",
    "test_generator=test_datagen.flow_from_directory('data/test/',\n",
    "                                                 target_size=(224,224),\n",
    "                                                 color_mode='rgb',\n",
    "                                                 batch_size=32,\n",
    "                                                 class_mode='categorical',\n",
    "                                                 shuffle=True)\n",
    "\n",
    "valid_datagen=ImageDataGenerator(preprocessing_function=preprocess_input) \n",
    "\n",
    "valid_generator=valid_datagen.flow_from_directory('data/valid/',\n",
    "                                                 target_size=(224,224),\n",
    "                                                 color_mode='rgb',\n",
    "                                                 batch_size=32,\n",
    "                                                 class_mode='categorical',\n",
    "                                                 shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "62/62 [==============================] - 703s 11s/step - loss: 0.4815 - acc: 0.8029 - val_loss: 2.1596 - val_acc: 0.5169\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.15959, saving model to model.weights.best.hdf5\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 648s 10s/step - loss: 0.4353 - acc: 0.8216 - val_loss: 2.7873 - val_acc: 0.5000\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 2.15959\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 649s 10s/step - loss: 0.4170 - acc: 0.8332 - val_loss: 1.9491 - val_acc: 0.4915\n",
      "\n",
      "Epoch 00003: val_loss improved from 2.15959 to 1.94912, saving model to model.weights.best.hdf5\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 610s 10s/step - loss: 0.3939 - acc: 0.8453 - val_loss: 2.3531 - val_acc: 0.5339\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.94912\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 584s 9s/step - loss: 0.3265 - acc: 0.8710 - val_loss: 2.6829 - val_acc: 0.5078\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.94912\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 577s 9s/step - loss: 0.2987 - acc: 0.8816 - val_loss: 1.7512 - val_acc: 0.4746\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.94912 to 1.75115, saving model to model.weights.best.hdf5\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 563s 9s/step - loss: 0.2993 - acc: 0.8826 - val_loss: 2.5846 - val_acc: 0.4661\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.75115\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 596s 10s/step - loss: 0.2191 - acc: 0.9168 - val_loss: 3.7644 - val_acc: 0.5085\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.75115\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 619s 10s/step - loss: 0.1742 - acc: 0.9385 - val_loss: 3.3151 - val_acc: 0.5085\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.75115\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 578s 9s/step - loss: 0.1735 - acc: 0.9375 - val_loss: 3.9001 - val_acc: 0.5312\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.75115\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21c165db438>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "step_size_train=train_generator.n//train_generator.batch_size\n",
    "step_size_valid = valid_generator.n//valid_generator.batch_size\n",
    "\n",
    "my_checkpoint = ModelCheckpoint(filepath='model.weights.best.hdf5', verbose=1, \n",
    "                               save_best_only=True)\n",
    "\n",
    "history = my_model.fit_generator(generator=train_generator,\n",
    "                       validation_data=valid_generator, validation_steps = step_size_valid,\n",
    "                       epochs=10, steps_per_epoch=step_size_train, callbacks=[my_checkpoint], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    " \n",
    "epochs = range(len(acc))\n",
    " \n",
    "plt.plot(epochs, acc, 'b', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    " \n",
    "plt.figure()\n",
    " \n",
    "plt.plot(epochs, loss, 'b', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "\n",
    "predictions = model.predict_generator(test_generator, steps=test_generator.samples/test_generator.batch_size,verbose=1)\n",
    "#predicted_classes = np.argmax(predictions,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('my_transfer.csv', 'w', newline='') as csvfile:\n",
    "    result_writer = csv.writer(csvfile)\n",
    "    result_writer.writerow(['Id', 'task_1', 'task_2'])\n",
    "    for test_filepath, test_prediction in zip(test_files, predictions):\n",
    "        result_writer.writerow([test_filepath, test_prediction[0][0], test_prediction[0][2]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
