1.	Decision Trees
One application of Decision Trees would be to diagnose a particular medical condition given the symptoms and test results as features and the diagnosis as the output
Decision trees, as a classifier are easy to understand. They can be represented without the use of complex models. Decision trees perform well when the interactions between the features is less complex [1].  Another advantage of Decision Trees is that they require no information about the distribution of the features [2].
Decision Trees are largely prone to overfitting [3]. Using large number of features which may not results in this behavior. Using a large number of features will also lead to a relatively large implementation.
Given the nature of our data where we have continuous attributes like hours-per-week and also nominal information like Race, Marital Status, etc., Decision Trees can handle both these type of inputs [1].  Decision Trees, are also, better resistant to irrelevant features in the data [4]. As is the case here, Marital Status, Relationship, Race, Sex, etc. may not affect the income of an individual. Decision Trees are also good at handling outliers in the data as we see from the skewed nature of capital-gain and capital-loss data.

2.	Random Forests (RF)
Random Forests can be used for a wide variety of applications. One such application, is the Gene selection and classification of microarray data as shown in [5].
RF classifiers can handle unbalanced data and also categorical data. They also prioritize features during the classification process.  [6]
The underlying learning concept when using Ensemble Methods may be difficult to understand when there are many features. Time taken for training and memory consumed will also be higher when compared to non-ensemble methods [7].
The high number of features in our dataset could lead to over fitting.  This can be overcome by using Random Forests. Also, some of the features appear to contain outliers. RF are generally immune to outliers. [8]

3. Gaussian Naive Bayes
GaussainNB classifiers have long been used for text retreival and classification. [9]
Naive Bayes algorithms in general are fast and require less memory. They are tolerant towards irrelevant features in the data. It works well on discrete and continuos data. [10]
Naive Bayes classifiers assume that the features are independent of each other, which may not be the case in some scenarios [9]. Also, GaussainNB performance will suffer if the data is not truly Gaussian.[11]  
The data, as we know, is both discrete and continuos valued. It also has many features, some of which could be less significant in predicting the output. Hence GaussianNB would be a suitable classifier to handle this data, whilst being fast and consuming less memory. 

[1] Rokach, L. and Maimon, O., 2005. Top-down induction of decision trees classifiers-a survey. IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews), 35(4), pp.476-487.
[2] Friedl, M.A., Brodley, C.E. and Strahler, A.H., 1999. Maximizing land cover classification accuracies produced by decision trees at continental to global scales. IEEE Transactions on Geoscience and Remote Sensing, 37(2), pp.969-977.
[3] Hailemariam, E., Goldstein, R., Attar, R. and Khan, A., 2011, April. Real-time occupancy detection using decision trees with multiple sensor types. In Proceedings of the 2011 Symposium on Simulation for Architecture and Urban Design (pp. 141-148). Society for Computer Simulation International.
[4] Hastie, T., Tibshirani, R. and Friedman, J., 2009. Boosting and Additive Trees. In The elements of statistical learning (pp. 350-351). Springer, New York, NY.
[5] DÃ­az-Uriarte, R. and De Andres, S.A., 2006. Gene selection and classification of microarray data using random forest. BMC bioinformatics, 7(1), p.3.
[6] Pal, M., 2005. Random forest classifier for remote sensing classification. International Journal of Remote Sensing, 26(1), pp.217-222.
[7] https://eecs.wsu.edu/~holder/courses/cse5361/spr06/lectures/ensemble.pdf 
[8] https://www.princeton.edu/~aylinc/files/CS613-15-04.pdf
[9] Lewis, D.D., 1998, April. Naive (Bayes) at forty: The independence assumption in information retrieval. In European conference on machine learning (pp. 4-15). Springer, Berlin, Heidelberg.
[10] http://www.cs.ucr.edu/~eamonn/CE/Bayesian%20Classification%20withInsect_examples.pdf
[11] http://responsive.media.mit.edu/wp-content/uploads/sites/5/2014/01/Class-4-Naive-Bayes.pdf

------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

The F-score for the testing set when 100% of the training set is used is comparable for all three classifiers with GaussainNB having a slightly lower score. The accuracy of GaussianNB is far lower compared to the other two classifiers. Decision Trees and RF require more training time but prediction time is far less for RF. This is a good bargain considering the final F-score. Since some features are more important than others and there are significant number of features, Random Forest will be most suited to this data rather than using Decision Tress, as this may result in overfitting.

------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

The final model chosen in this case, is Random Forests. It can be called an extension or superset of small decision trees. A single Decision Tree would incorporate all the features in its rules which may result in overfitting. Random Forests work around this problem by picking random subsets of features and forming Decision Trees for these subsets. They final decision from these small trees are then weighted based on the importance of the features to obtain the final vote / prediction for the problem. In short, Random Forests are an intelligent combination of small Decision Trees, whose decisions are weighted to arrive at a final prediction.

------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

The accuracy and F-score of the optimized model on the testing data is 0.8603 and 0.7279 respectively. These results are better when compared to the unoptimized model. In comparison with the Naive Predictor, the results of the optimized model show a huge improvement. There is nearly a three-fold improvement in both the accuracy and f-score. However, if the Naive Predictor were to predict all persons as having income less than $50K, then its acccuracy would be much higer and not so further from the unoptimized model.

------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

The five features that would be important are: age, education_level, workclass, occupation and hours-per-week.

Rank 1: Age - With age being higher, the person likely to have more experience and thereby a higher income.
Rank 2: Education Level - A person with higher education levels is likely to have a higher income.
Rank 3: Occupation - The levels of income are dependent on the field / nature of job the individual is involved in.
Rank 4: Captial Gain - Income arising from capital gains (stock, equity, real-estate) could play a significant role in increasing / decreasing an individuals annual income.
Rank 4: Hours Per Week - In the case of hourly employment or self-employed individuals, the number of hours worked per week is likely to have a direct influence on their income. 

------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Age, Capital Gain and Hours Per Week are part of the answer for Question 6. Relationship(Husband), Marital Status(civ-spouse) are the features that are not discussed in the previous answer. 
These two features are significant because the relationship or marital status of the person is a key factor in deciding the income a person needs to support his / her family. Thus they could be more imporant features in this particular case.
Overall, these five features account for 60% of the importance in deciding the annual income of a person.

------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

The accuracy and f-score of the final model are slightly lesser than that of that model that uses all the features. The accuracy is lowered by about 6% whereas the f-score drops by nearly 15%. The training time taken by the model utilizing the full set of features is far more than the one using the reduced data. Thus, if training time was a factor, it would be a good trade-off to use the reduced model with an accuracy that is only 5% lower than the accuracy of the full model.